name: Run all scrapers and merge (PR auto-merge)

on:
  workflow_dispatch:
  schedule:
    - cron: '0 */12 * * *'

permissions:
  contents: write
  pull-requests: write

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements_scraper.txt

      - name: Run all scrapers
        env:
          SERPER_API_KEY: ${{ secrets.SERPER_API_KEY }}
        run: python run_all.py

      - name: Create Pull Request with updated CSV
        id: cpr
        uses: peter-evans/create-pull-request@v6
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          commit-message: "Auto update merged_jobs.csv"
          title: "Auto update merged_jobs.csv"
          body: |
            Automated update of merged_jobs.csv generated by GitHub Actions.
          branch: automation/update-csv
          branch-suffix: timestamp
          add-paths: |
            merged_jobs.csv
          labels: |
            automation
            data-update
          delete-branch: true

      - name: Enable auto-merge
        if: ${{ steps.cpr.outputs['pull-request-number'] != '' }}
        uses: peter-evans/enable-pull-request-automerge@v3
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          pull-request-number: ${{ steps.cpr.outputs['pull-request-number'] }}
          merge-method: squash

      - name: Show PR URL (if created)
        if: ${{ steps.cpr.outputs['pull-request-url'] != '' }}
        run: |
          echo "Created PR: ${{ steps.cpr.outputs['pull-request-url'] }}"
